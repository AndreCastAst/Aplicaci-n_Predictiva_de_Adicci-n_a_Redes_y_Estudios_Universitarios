# -*- coding: utf-8 -*-
"""Proyecto de predicci√≥n de adicci√≥n.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ynx6_939piEORA9L8qU7CC6-gqjLR6Vm
"""







"""# Tarea
Se analiz√≥ el conjunto de datos 'Estudiantes_Adiccion_Redes_Sociales_sin_tildes.xlsx' para predecir 'Afecta_Rendimiento' preprocesando los datos, aplicando y evaluando modelos de Regresi√≥n Lineal/Log√≠stica, J48 (√Årbol de Decisi√≥n) y Naive Bayes, luego comparando su desempe√±o para identificar el mejor modelo.

## Cargar Dataset

### Subtarea:
Cargar el archivo 'Estudiantes_Adiccion_Redes_Sociales_sin_tildes.xlsx' en un DataFrame de pandas.

Para cargar el archivo Excel, primero se necesita importar la biblioteca pandas, que es una biblioteca fundamental para la manipulaci√≥n de datos en Python.
"""

import pandas as pd
print("pandas importado como pd")

"""Ahora que pandas est√° importado, se cargar√° el archivo Excel 'Estudiantes_Adiccion_Redes_Sociales_sin_tildes.xlsx' en un DataFrame llamado df_social_media_addictiony se mostrar√° sus primeras filas para verificar la carga.


"""

df_social_media_addiction = pd.read_csv('Estudiantes_Adiccion_Redes_Sociales (2).csv')
print("DataFrame 'df_social_media_addiction' loaded successfully.")
df_social_media_addiction.head()



"""# Tarea
Se realiz√≥ una exploraci√≥n de datos inicial en el df_social_media_addictionDataFrame para comprender su estructura, tipos de datos e identificar cualquier valor faltante, como siguiente paso en el preprocesamiento de datos.

## Realizar la exploraci√≥n inicial de datos

Para comprender la estructura del DataFrame, los tipos de datos e identificar los valores faltantes, se utiliz√≥ df.info()para un resumen conciso, df.describe()para estad√≠sticas descriptivas de columnas num√©ricas y df.isnull().sum()para contar los valores faltantes por columna.
"""

print("DataFrame Information:")
df_social_media_addiction.info()

print("\nDescriptive Statistics:")
df_social_media_addiction.describe()

print("\nMissing Values per Column:")
df_social_media_addiction.isnull().sum()

"""## Convertir variable objetivo y codificar variables categ√≥ricas

### Subtask:
Convertir la variable objetivo 'Afecta_Rendimiento_Academico' a un formato num√©rico y aplicar codificaci√≥n One-Hot a las dem√°s variables categ√≥ricas.

Primero, se converti√≥ la variable de destino ¬´Afecta_Rendimiento_Acad√©mico¬ª a un formato num√©rico (1 para ¬´S√≠¬ª, 0 para ¬´No¬ª). Luego, se identific√≥ todas las dem√°s columnas categ√≥ricas (tipo de objeto) y se aplic√≥ la codificaci√≥n One-Hot, eliminando las columnas originales para preparar el conjunto de datos para el modelado.
"""

print("Original unique values for 'Afecta_Rendimiento_Academico':", df_social_media_addiction['Afecta_Rendimiento_Academico'].unique())

df_social_media_addiction['Afecta_Rendimiento_Academico'] = df_social_media_addiction['Afecta_Rendimiento_Academico'].map({'Si': 1, 'No': 0})

print("Converted unique values for 'Afecta_Rendimiento_Academico':", df_social_media_addiction['Afecta_Rendimiento_Academico'].unique())

categorical_cols = df_social_media_addiction.select_dtypes(include=['object']).columns.tolist()

if 'ID_Estudiante' in categorical_cols:
    categorical_cols.remove('ID_Estudiante')

print("\nCategorical columns identified for One-Hot Encoding (excluding ID_Estudiante):", categorical_cols)

df_social_media_addiction = pd.get_dummies(df_social_media_addiction, columns=categorical_cols, drop_first=True, dtype=int)

print("\nDataFrame after One-Hot Encoding:")
df_social_media_addiction.head()
print("\nUpdated DataFrame Info:")
df_social_media_addiction.info()

"""## Preparar datos para el modelado: Dividir en conjuntos de entrenamiento y prueba

### Subtask:
Separar la variable objetivo 'Afecta_Rendimiento_Academico' de las caracter√≠sticas y dividir el conjunto de datos en conjuntos de entrenamiento y prueba.

Para preparar los datos para el modelado, primero se separar√≥ la variable objetivo ¬´Afecta_Rendimiento_Acad√©mico¬ª de las caracter√≠sticas. Luego, se dividi√≥ el conjunto de datos en conjuntos de entrenamiento y de prueba train_test_split para asegurar una evaluaci√≥n robusta de los modelos. Finalmente, se imprimi√≥ las formas de los conjuntos de datos resultantes para verificar la divisi√≥n.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib # Added for saving objects
import pandas as pd # Ensure pandas is imported

# Check if df_social_media_addiction is defined
if 'df_social_media_addiction' not in locals() and 'df_social_media_addiction' not in globals():
    print("Error: DataFrame 'df_social_media_addiction' not found.\nPlease ensure that previous cells related to data loading and preprocessing (e.g., cell 9d34a906 and 6753f37a) have been executed.")
else:
    # Define the target variable y
    y = df_social_media_addiction['Afecta_Rendimiento_Academico']

    # Define the feature matrix X, dropping 'ID_Estudiante' and the target variable
    X = df_social_media_addiction.drop(columns=['ID_Estudiante', 'Afecta_Rendimiento_Academico'])

    # Get feature column names before splitting and scaling for later use in prediction interface
    feature_columns = X.columns.tolist()

    # Split the dataset into 70% training and 30% temporary set
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

    # Split the temporary set into 2/3 test and 1/3 validation (20% test, 10% validation of original)
    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp)

    # Identify numerical columns (excluding one-hot encoded and target which are already handled)
    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns

    # Calculate mean of numerical columns from the training set BEFORE scaling, for interactive interface defaults
    numerical_means = X_train[numerical_cols].mean().to_dict()

    scaler = StandardScaler()
    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])
    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])

    # Save the scaler, feature columns, and numerical means for later use in the prediction interface
    joblib.dump(scaler, 'scaler.pkl')
    joblib.dump(feature_columns, 'feature_columns.pkl')
    joblib.dump(numerical_means, 'numerical_means.pkl')

    # Print the shapes of the resulting sets to verify the split
    print("Shape of X_train:", X_train.shape)
    print("Shape of X_test:", X_test.shape)
    print("Shape of X_val:", X_val.shape)
    print("Shape of y_train:", y_train.shape)
    print("Shape of y_test:", y_test.shape)
    print("Shape of y_val:", y_val.shape)

"""## Entrenar y evaluar el modelo de Regresi√≥n Log√≠stica

### Subtask:
Entrenar un modelo de Regresi√≥n Log√≠stica usando los datos de entrenamiento y evaluar su rendimiento.

para entrenar un modelo de regresi√≥n log√≠stica,se necesit√≥ importar la LogisticRegressionclase, instanciarla, ajustarla a los datos de entrenamiento y luego hacer predicciones en el conjunto de prueba seg√∫n las instrucciones.
"""

from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression model instance with random_state for reproducibility
log_reg_model = LogisticRegression(random_state=42, solver='liblinear') # Added solver for older sklearn versions

# Fit the model to the training data
print("Training Logistic Regression model...")
log_reg_model.fit(X_train, y_train)
print("Logistic Regression model trained.")

# Make predictions on the test set
y_pred_lr = log_reg_model.predict(X_test)
print("Predictions made on the test set.")

"""Una vez entrenado el modelo de regresi√≥n log√≠stica y y_pred_lrrealizadas las predicciones ( ) en el conjunto de prueba, el siguiente paso es evaluar su rendimiento. Se Utiliz√≥ m√©tricas de clasificaci√≥n comunes, como exactitud, precisi√≥n, recuperaci√≥n y puntuaci√≥n F1, para evaluar el rendimiento del modelo.

"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Evaluate the model
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)

print(f"\nLogistic Regression Model Performance:")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-Score: {f1_lr:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))

"""## Entrenar y evaluar el modelo de √Årbol de Decisi√≥n (J48)

### Subtask:
Entrenar un modelo de √Årbol de Decisi√≥n usando los datos de entrenamiento y evaluar su rendimiento.

Para entrenar un modelo de √°rbol de decisiones y evaluar su desempe√±o, necesito importar la DecisionTreeClassifierclase, instanciarla con un random_state, ajustarla a los datos de entrenamiento, hacer predicciones en el conjunto de prueba y luego calcular y mostrar varias m√©tricas de clasificaci√≥n, incluidas precisi√≥n, exactitud, recuperaci√≥n, puntaje F1, un informe de clasificaci√≥n y una matriz de confusi√≥n.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Create a Decision Tree classifier instance with random_state for reproducibility
dt_model = DecisionTreeClassifier(random_state=42)

# Fit the model to the training data
print("Training Decision Tree model...")
dt_model.fit(X_train, y_train)
print("Decision Tree model trained.")

# Make predictions on the test set
y_pred_dt = dt_model.predict(X_test)
print("Predictions made on the test set.")

# Evaluate the model
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)

print(f"\nDecision Tree Model Performance:")
print(f"Accuracy: {accuracy_dt:.4f}")
print(f"Precision: {precision_dt:.4f}")
print(f"Recall: {recall_dt:.4f}")
print(f"F1-Score: {f1_dt:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))

"""## Entrenar y evaluar el modelo de Naive Bayes

### Subtask:
Entrenar un modelo de Naive Bayes usando los datos de entrenamiento y evaluar su rendimiento.

Para entrenar un modelo Naive Bayes y evaluar su desempe√±o, importar√© la GaussianNBclase y las m√©tricas de evaluaci√≥n necesarias, instanciar√© el modelo, lo ajustar√© a los datos de entrenamiento, har√© predicciones en el conjunto de prueba y luego calcular√© y mostrar√© la exactitud, precisi√≥n, recuperaci√≥n, puntaje F1, un informe de clasificaci√≥n y una matriz de confusi√≥n.
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Create a Gaussian Naive Bayes model instance
nb_model = GaussianNB()

# Fit the model to the training data
print("Training Gaussian Naive Bayes model...")
nb_model.fit(X_train, y_train)
print("Gaussian Naive Bayes model trained.")

# Make predictions on the test set
y_pred_nb = nb_model.predict(X_test)
print("Predictions made on the test set.")

# Evaluate the model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
precision_nb = precision_score(y_test, y_pred_nb)
recall_nb = recall_score(y_test, y_pred_nb)
f1_nb = f1_score(y_test, y_pred_nb)

print(f"\nGaussian Naive Bayes Model Performance:")
print(f"Accuracy: {accuracy_nb:.4f}")
print(f"Precision: {precision_nb:.4f}")
print(f"Recall: {recall_nb:.4f}")
print(f"F1-Score: {f1_nb:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_nb))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_nb))

"""## Comparar el rendimiento de los modelos

### Subtask:
Comparar el rendimiento de los modelos de Regresi√≥n Log√≠stica, √Årbol de Decisi√≥n y Naive Bayes utilizando las m√©tricas de evaluaci√≥n.

Para comparar los modelos, crear√© un DataFrame de pandas que resuma las m√©tricas de rendimiento (precisi√≥n, exactitud, recuperaci√≥n, puntuaci√≥n F1) para cada modelo (regresi√≥n log√≠stica, √°rbol de decisi√≥n y Bayes ingenuo) utilizando los valores ya calculados y luego mostrar√© el DataFrame.
"""

import pandas as pd

# Create a dictionary to store the performance metrics
model_performance = {
    'Model': ['Logistic Regression', 'Decision Tree', 'Naive Bayes'],
    'Accuracy': [accuracy_lr, accuracy_dt, accuracy_nb],
    'Precision': [precision_lr, precision_dt, precision_nb],
    'Recall': [recall_lr, recall_dt, recall_nb],
    'F1-Score': [f1_lr, f1_dt, f1_nb]
}

# Create a DataFrame from the dictionary
df_performance = pd.DataFrame(model_performance)

# Display the DataFrame
print("\nComparison of Model Performance:")
print(df_performance.to_markdown(index=False))

"""##Identificar el mejor modelo y concluir el an√°lisis

Analice la comparaci√≥n de los rendimientos de los modelos para identificar el modelo con mejor rendimiento y resumir los hallazgos generales.

## Visualizaci√≥n de Curvas ROC y M√©tricas Relacionadas

Para una evaluaci√≥n m√°s profunda de los modelos, visualizaremos las curvas ROC (Receiver Operating Characteristic) y calcularemos el √Årea bajo la Curva (AUC) para la Regresi√≥n Log√≠stica, el √Årbol de Decisi√≥n y Naive Bayes. Esto nos permitir√° entender mejor el rendimiento de cada clasificador en diferentes umbrales de decisi√≥n.
"""

from sklearn.metrics import RocCurveDisplay, roc_auc_score
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))

# Logistic Regression ROC Curve
y_prob_lr = log_reg_model.predict_proba(X_test)[:, 1]
RocCurveDisplay.from_estimator(log_reg_model, X_test, y_test, name='Regresi√≥n Log√≠stica', ax=plt.gca(), plot_chance_level=False)
auc_lr = roc_auc_score(y_test, y_prob_lr)

# Decision Tree ROC Curve
y_prob_dt = dt_model.predict_proba(X_test)[:, 1]
RocCurveDisplay.from_estimator(dt_model, X_test, y_test, name='√Årbol de Decisi√≥n', ax=plt.gca(), plot_chance_level=False)
auc_dt = roc_auc_score(y_test, y_prob_dt)

# Naive Bayes ROC Curve
y_prob_nb = nb_model.predict_proba(X_test)[:, 1]
RocCurveDisplay.from_estimator(nb_model, X_test, y_test, name='Naive Bayes', ax=plt.gca(), plot_chance_level=False)
auc_nb = roc_auc_score(y_test, y_prob_nb)

# Plot 'No Skill' line
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='No Skill (AUC = 0.5)')

plt.title('Curvas ROC de los Modelos de Clasificaci√≥n')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

print(f"\nAUC para Regresi√≥n Log√≠stica: {auc_lr:.4f}")
print(f"AUC para √Årbol de Decisi√≥n: {auc_dt:.4f}")
print(f"AUC para Naive Bayes: {auc_nb:.4f}")

"""## Visualizaci√≥n de M√©tricas de Rendimiento

Para una mejor comprensi√≥n del rendimiento de los modelos, visualizaremos las matrices de confusi√≥n y las m√©tricas clave (Accuracy, Precision, Recall, F1-Score) para los modelos de √Årbol de Decisi√≥n y Naive Bayes.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# --- Confusion Matrix for Decision Tree ---
cm_dt = confusion_matrix(y_test, y_pred_dt)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Afecta', 'Afecta'], yticklabels=['No Afecta', 'Afecta'])
plt.title('Matriz de Confusi√≥n - √Årbol de Decisi√≥n')
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.show()

# --- Confusion Matrix for Naive Bayes ---
cm_nb = confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Afecta', 'Afecta'], yticklabels=['No Afecta', 'Afecta'])
plt.title('Matriz de Confusi√≥n - Naive Bayes')
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.show()

# --- Comparison of Metrics (Bar Plot) ---
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
dt_scores = [accuracy_dt, precision_dt, recall_dt, f1_dt]
nb_scores = [accuracy_nb, precision_nb, recall_nb, f1_nb]
lr_scores = [accuracy_lr, precision_lr, recall_lr, f1_lr]

x = range(len(metrics))
width = 0.2

plt.figure(figsize=(12, 6))
plt.bar([i - width for i in x], lr_scores, width, label='Regresi√≥n Log√≠stica', color='skyblue')
plt.bar(x, dt_scores, width, label='√Årbol de Decisi√≥n', color='lightcoral')
plt.bar([i + width for i in x], nb_scores, width, label='Naive Bayes', color='lightgreen')

plt.ylabel('Puntuaci√≥n')
plt.title('Comparaci√≥n de M√©tricas de Rendimiento del Modelo')
plt.xticks(x, metrics)
plt.ylim(0, 1.1) # Set y-axis limit to 0-1.1 for scores
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""##Analizar la comparaci√≥n de los rendimientos de los modelos para identificar el modelo con mejor rendimiento y resumir los hallazgos generales.
Desde el df_performanceDataFrame, observamos lo siguiente:

| Model               | Accuracy | Precision | Recall | F1-Score |
|:--------------------|---------:|----------:|-------:|---------:|
| Logistic Regression | 1.000000 | 1.000000  | 1.0000 | 1.0000   |
| Decision Tree       | 1.000000 | 1.000000  | 1.0000 | 1.0000   |
| Naive Bayes         | 0.730496 | 0.964912  | 0.6044 | 0.7432   |

Tanto el modelo de regresi√≥n log√≠stica como el de √°rbol de decisi√≥n obtuvieron puntuaciones perfectas en todas las m√©tricas evaluadas (exactitud, precisi√≥n, recuperaci√≥n y puntuaci√≥n F1), con un valor de 1,00. Esto indica que estos modelos clasificaron correctamente todas las instancias del conjunto de prueba sin errores, lo que sugiere un ajuste muy s√≥lido a los datos y una excelente capacidad predictiva para este conjunto de datos en particular.

El modelo Naive Bayes , si bien mostr√≥ una alta precisi√≥n de 0,965, tuvo un rendimiento significativamente inferior al de los otros dos modelos, con una exactitud de 0,730, una recuperaci√≥n de 0,604 y una puntuaci√≥n F1 de 0,743. Su menor recuperaci√≥n indica que no detect√≥ una proporci√≥n considerable de los casos positivos.

Conclusi√≥n: Tanto el modelo de regresi√≥n log√≠stica como el de √°rbol de decisi√≥n son los modelos con mejor rendimiento para este conjunto de datos, alcanzando un 100 % de exactitud, precisi√≥n, recuperaci√≥n y puntuaci√≥n F1 en el conjunto de prueba. Esto sugiere que las caracter√≠sticas dise√±adas (especialmente mediante la codificaci√≥n One-Hot de variables categ√≥ricas) fueron muy eficaces para distinguir entre las dos clases de estos modelos. El modelo Naive Bayes, si bien tuvo un rendimiento razonablemente bueno, fue superado por los otros dos, probablemente debido a que sus supuestos subyacentes no se ajustaban tan bien a las caracter√≠sticas del conjunto de datos o a la naturaleza altamente discriminante de las caracter√≠sticas transformadas.

##Tarea final

El an√°lisis est√° completo. Resuma los hallazgos sobre el rendimiento del modelo e identifique el/los mejor(es).

##Resumen:
#Preguntas y respuestas
El an√°lisis identific√≥ que tanto el modelo de regresi√≥n log√≠stica como el de √°rbol de decisi√≥n obtuvieron un rendimiento √≥ptimo en el conjunto de prueba, con valores de exactitud, precisi√≥n, recuperaci√≥n y puntuaci√≥n F1 de 1,0000. El modelo Naive Bayes tuvo un rendimiento significativamente inferior, con una exactitud de 0,7305, una precisi√≥n de 0,9649, una recuperaci√≥n de 0,6044 y una puntuaci√≥n F1 de 0,7432. Por lo tanto, la regresi√≥n log√≠stica y el √°rbol de decisi√≥n se identifican como los mejores modelos seg√∫n las m√©tricas evaluadas.

#Hallazgos clave del an√°lisis de datos
El conjunto de datos inicial, df_social_media_addiction, conten√≠a 705 entradas y 13 columnas, que constaban de 5 tipos de datos enteros, 2 flotantes y 6 de objeto, sin valores faltantes.
La variable de destino, Afecta_Rendimiento_Academico, se convirti√≥ exitosamente de categ√≥rica ('Si', 'No') a num√©rica (1, 0).
Se codificaron cinco columnas categ√≥ricas ( Genero, Nivel_Academico, Pais, Plataforma_Mas_Usada, ) con One-Hot, lo que dio como resultado un DataFrame transformado con 131 caracter√≠sticas.Estado_Civil
El conjunto de datos se dividi√≥ en conjuntos de entrenamiento (564 muestras) y de prueba (141 muestras) utilizando una proporci√≥n 80/20, con estratificaci√≥n aplicada a la variable objetivo para mantener su distribuci√≥n.
Modelo de regresi√≥n log√≠stica : se logr√≥ un rendimiento perfecto en el conjunto de pruebas con una exactitud de 1,0000, una precisi√≥n de 1,0000, una recuperaci√≥n de 1,0000 y una puntuaci√≥n F1 de 1,0000.
Modelo de √°rbol de decisiones : tambi√©n logr√≥ un rendimiento perfecto en el conjunto de pruebas con una exactitud de 1,0000, una precisi√≥n de 1,0000, una recuperaci√≥n de 1,0000 y una puntuaci√≥n F1 de 1,0000.
Modelo Naive Bayes : mostr√≥ un rendimiento considerablemente menor en comparaci√≥n con los otros dos, con una precisi√≥n de 0,7305, una precisi√≥n de 0,9649, una recuperaci√≥n de 0,6044 y un puntaje F1 de 0,7432.
Perspectivas o pr√≥ximos pasos
El rendimiento √≥ptimo de los modelos de regresi√≥n log√≠stica y √°rbol de decisi√≥n en el conjunto de prueba sugiere un conjunto de datos altamente separable o un posible sobreajuste o fuga de datos. Se recomienda una validaci√≥n adicional con t√©cnicas como la validaci√≥n cruzada de k-fold para confirmar la generalizaci√≥n de estos modelos.
Dada la diferencia significativa en el rendimiento entre los modelos, investigar la importancia de las caracter√≠sticas de los modelos de mayor rendimiento (Regresi√≥n log√≠stica y √Årbol de decisi√≥n) podr√≠a brindar informaci√≥n valiosa sobre qu√© factores influyen fuertemente en la predicci√≥n del rendimiento acad√©mico afectado por el uso de las redes sociales.

# Task
A√±adir una celda de texto explicando la importancia y el procedimiento de la validaci√≥n cruzada k-fold para una evaluaci√≥n m√°s robusta del modelo.

## Introducir K-Fold Cross-Validation

### Subtask:
A√±adir una celda de texto explicando la importancia y el procedimiento de la validaci√≥n cruzada k-fold para una evaluaci√≥n m√°s robusta del modelo.

La **validaci√≥n cruzada k-fold** es una t√©cnica esencial para evaluar la robustez y la generalizaci√≥n de un modelo predictivo, especialmente cuando el rendimiento observado en un √∫nico conjunto de prueba podr√≠a ser sesgado o poco representativo. Su importancia radica en que ayuda a mitigar el sobreajuste y proporciona una estimaci√≥n m√°s fiable del rendimiento del modelo en datos no vistos.

### Importancia:
*   **Evaluaci√≥n m√°s robusta**: Al dividir el conjunto de datos en m√∫ltiples subconjuntos y rotar el uso de estos para entrenamiento y validaci√≥n, se obtiene una medida m√°s estable del rendimiento del modelo, reduciendo la varianza inherente a una √∫nica partici√≥n de entrenamiento/prueba.
*   **Uso eficiente de los datos**: Permite utilizar todos los datos disponibles tanto para entrenamiento como para validaci√≥n, lo cual es crucial en conjuntos de datos peque√±os o medianos donde una divisi√≥n √∫nica podr√≠a dejar muy pocos datos para una de las fases.
*   **Detecci√≥n de sobreajuste**: Ayuda a identificar si el modelo est√° aprendiendo patrones espec√≠ficos del conjunto de entrenamiento en lugar de generalizar a nuevos datos.

### Procedimiento:
1.  **Divisi√≥n del dataset**: El conjunto de datos original se divide en 'k' subconjuntos (folds) de tama√±o aproximadamente igual. Por ejemplo, si k=5, el dataset se divide en 5 partes.
2.  **Iteraci√≥n**: El proceso se repite 'k' veces (o 'k' iteraciones).
    *   En cada iteraci√≥n, uno de los 'k' folds se designa como el conjunto de **validaci√≥n**.
    *   Los 'k-1' folds restantes se combinan para formar el conjunto de **entrenamiento**.
3.  **Entrenamiento y evaluaci√≥n**: El modelo se entrena con el conjunto de entrenamiento de esa iteraci√≥n y se eval√∫a con el conjunto de validaci√≥n.
4.  **Recopilaci√≥n de resultados**: Se registran las m√©tricas de rendimiento (como precisi√≥n, recall, F1-score, AUC) obtenidas en la evaluaci√≥n.
5.  **Promedio de resultados**: Una vez completadas las 'k' iteraciones, se promedian las m√©tricas de rendimiento recopiladas. Este promedio representa la estimaci√≥n final y m√°s robusta del rendimiento del modelo.

## K-Fold Cross-Validation para Regresi√≥n Log√≠stica

### Subtask:
Realizar validaci√≥n cruzada k-fold (por ejemplo, con 5 o 10 folds) para el modelo de Regresi√≥n Log√≠stica, calculando las m√©tricas promedio de Accuracy, Precision, Recall, F1-Score y ROC AUC.

Para realizar una validaci√≥n cruzada de k pliegues para el modelo de regresi√≥n log√≠stica, necesito importar los m√≥dulos necesarios, inicializar el modelo, definir la estrategia de validaci√≥n cruzada, calcular varias m√©tricas de rendimiento en los pliegues y luego informar sus promedios y desviaciones est√°ndar.
"""

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
import numpy as np

# Initialize a Logistic Regression model
log_reg_kfold_model = LogisticRegression(random_state=42, solver='liblinear')

# Define the cross-validation strategy
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation for Accuracy, Precision, Recall, F1-Score
accuracy_scores = cross_val_score(log_reg_kfold_model, X, y, cv=kf, scoring='accuracy')
precision_scores = cross_val_score(log_reg_kfold_model, X, y, cv=kf, scoring='precision')
recall_scores = cross_val_score(log_reg_kfold_model, X, y, cv=kf, scoring='recall')
f1_scores = cross_val_score(log_reg_kfold_model, X, y, cv=kf, scoring='f1')

# Calculate ROC AUC manually for each fold as cross_val_score doesn't directly support predict_proba for AUC
roc_auc_scores = []
for train_index, test_index in kf.split(X, y):
    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

    # Apply StandardScaler to the folds - re-fit on train_fold and transform both
    scaler_kfold = StandardScaler()
    X_train_fold_scaled = scaler_kfold.fit_transform(X_train_fold)
    X_test_fold_scaled = scaler_kfold.transform(X_test_fold)

    log_reg_kfold_model.fit(X_train_fold_scaled, y_train_fold)
    y_pred_proba = log_reg_kfold_model.predict_proba(X_test_fold_scaled)[:, 1]
    roc_auc_scores.append(roc_auc_score(y_test_fold, y_pred_proba))

# Calculate mean and standard deviation for each metric
print(f"Logistic Regression K-Fold Cross-Validation (n_splits=5):")
print(f"  Accuracy: {np.mean(accuracy_scores):.4f} (+/- {np.std(accuracy_scores):.4f})")
print(f"  Precision: {np.mean(precision_scores):.4f} (+/- {np.std(precision_scores):.4f})")
print(f"  Recall: {np.mean(recall_scores):.4f} (+/- {np.std(recall_scores):.4f})")
print(f"  F1-Score: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})")
print(f"  ROC AUC: {np.mean(roc_auc_scores):.4f} (+/- {np.std(roc_auc_scores):.4f})")

import joblib

# Guardamos el modelo de Regresi√≥n Log√≠stica
joblib.dump(log_reg_kfold_model, 'modelo_rendimiento.pkl')

print("Modelo guardado exitosamente.")

# Instalamos la librer√≠a de widgets
!pip install ipywidgets --quiet

# Importamos lo necesario
import joblib
import numpy as np
import pandas as pd
from ipywidgets import interact, FloatSlider, IntSlider, Output
from IPython.display import display

# Cargar el modelo guardado
modelo_cargado = joblib.load('modelo_rendimiento.pkl')

# Cargar el scaler, las columnas de caracter√≠sticas y las medias num√©ricas
scaler_cargado = joblib.load('scaler.pkl')
feature_columns_cargadas = joblib.load('feature_columns.pkl')
numerical_means_cargadas = joblib.load('numerical_means.pkl')

# Cargar el DataFrame original para obtener los rangos min/max para los sliders
if 'df_social_media_addiction' not in locals() and 'df_social_media_addiction' not in globals():
    try:
        df_original = pd.read_csv('Estudiantes_Adiccion_Redes_Sociales.csv') # Corrected filename based on previous fix
        # Convert 'Afecta_Rendimiento_Academico' to numeric as it is done in preprocessing
        df_original['Afecta_Rendimiento_Academico'] = df_original['Afecta_Rendimiento_Academico'].map({'S√≠': 1, 'No': 0})
        categorical_cols_original = df_original.select_dtypes(include=['object']).columns.tolist()
        if 'ID_Estudiante' in categorical_cols_original:
            categorical_cols_original.remove('ID_Estudiante')
        df_original = pd.get_dummies(df_original, columns=categorical_cols_original, drop_first=True, dtype=int)

    except FileNotFoundError:
        print("Error: 'Estudiantes_Adiccion_Redes_Sociales.csv' no encontrado. Aseg√∫rate de que el archivo existe o de que las celdas de carga de datos se han ejecutado.")
        df_original = pd.DataFrame() # Fallback a un DataFrame vac√≠o para evitar errores
else:
    df_original = df_social_media_addiction # Usar el DF ya preprocesado y one-hot-encoded

# Definir los rangos para los sliders basados en el DataFrame original
min_puntaje_adiccion = df_original['Puntaje_Adiccion'].min() if not df_original.empty else 0
max_puntaje_adiccion = df_original['Puntaje_Adiccion'].max() if not df_original.empty else 10
min_horas_sueno = df_original['Horas_Sueno_Por_Noche'].min() if not df_original.empty else 4.0
max_horas_sueno = df_original['Horas_Sueno_Por_Noche'].max() if not df_original.empty else 10.0
min_conflictos_redes = df_original['Conflictos_Por_Redes_Sociales'].min() if not df_original.empty else 0
max_conflictos_redes = df_original['Conflictos_Por_Redes_Sociales'].max() if not df_original.empty else 5


# Inicializar un contenedor para la salida
salida = Output()

def predecir_rendimiento_academico(puntaje_adiccion, horas_sueno, conflictos_redes):
    """Funci√≥n para generar la predicci√≥n basada en los inputs del usuario."""

    # 1. Crear la entrada (Vector de caracter√≠sticas)
    # Inicializar con las medias de entrenamiento para todas las caracter√≠sticas
    nueva_instancia_dict = numerical_means_cargadas.copy()

    # Actualizar los valores de las caracter√≠sticas controladas por los sliders
    nueva_instancia_dict['Puntaje_Adiccion'] = puntaje_adiccion
    nueva_instancia_dict['Horas_Sueno_Por_Noche'] = horas_sueno
    nueva_instancia_dict['Conflictos_Por_Redes_Sociales'] = conflictos_redes

    # Convertir el diccionario a un DataFrame con una sola fila
    # Asegurar el orden de las columnas seg√∫n feature_columns_cargadas
    nueva_instancia_df = pd.DataFrame([nueva_instancia_dict], columns=feature_columns_cargadas)

    # 2. Aplicar el scaler
    nueva_instancia_escalada = scaler_cargado.transform(nueva_instancia_df)

    # 3. Generar la predicci√≥n
    prediccion = modelo_cargado.predict(nueva_instancia_escalada)[0]

    # 4. Mostrar el resultado
    salida.clear_output()
    with salida:
        if prediccion == 1:
            print("üö® Predicci√≥n: El rendimiento acad√©mico S√ç ser√° afectado.")
        else:
            print("‚úÖ Predicci√≥n: El rendimiento acad√©mico NO ser√° afectado.")

interact(predecir_rendimiento_academico,
         puntaje_adiccion=IntSlider(min=min_puntaje_adiccion, max=max_puntaje_adiccion, step=1, description='Puntaje Adicci√≥n:'),
         horas_sueno=FloatSlider(min=min_horas_sueno, max=max_horas_sueno, step=0.5, description='Horas de Sue√±o:'),
         conflictos_redes=IntSlider(min=min_conflictos_redes, max=max_conflictos_redes, step=1, description='Conflictos por Redes:')
        )

display(salida)

from google.colab import files
files.download("modelo_rendimiento.pkl")